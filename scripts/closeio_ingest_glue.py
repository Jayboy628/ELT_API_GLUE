# ==== Close.io → S3 (Glue Notebook, NO FUNCTIONS) ====
# Glue 4.0 / PySpark. Pagination for leads + activities. API key from Secrets Manager.

import os, json, time, base64, random, traceback, datetime as dt
from typing import Dict, Any, List, Optional
import boto3
import urllib3
from pyspark.sql import functions as F

# -----------------------------
# CONFIG (edit these)
# -----------------------------
REGION             = os.environ.get("AWS_REGION", "us-east-1")
SECRET_ID          = "api_key_closeio-dev"   # Secrets Manager name or ARN; secret = raw string or {"api_key":"..."}
API_BASE_URL       = "https://api.close.com/api/v1"
S3_OUTPUT          = "s3://lead-elt-657082399901-dev/landing/closeio/"  # bucket/prefix
OUTPUT_FORMAT      = "parquet"               # "parquet" or "json"

START_DATE         = ""                      # "YYYY-MM-DD" (inclusive). If "", defaults to yesterday
END_DATE           = ""                      # "YYYY-MM-DD" (inclusive). If "", defaults to today

PAGE_SIZE          = 200                     # leads search page size (cap at 200)
MAX_PAGES          = 500                     # safety cap per-day search pages
FETCH_LEAD_DETAILS = False                   # also fetch /lead/{id}/
LEAD_LIMIT         = 0                       # 0 = no cap; otherwise first N leads

ACT_FLUSH_EVERY    = 10_000                  # flush activities to S3 every N records
LEAD_FLUSH_EVERY   = 1_000                   # flush lead details to S3 every N records

# -----------------------------
# Spark / Glue session
# -----------------------------
try:
    from pyspark.context import SparkContext
    from awsglue.context import GlueContext
    sc = SparkContext.getOrCreate()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
except Exception:
    from pyspark.sql import SparkSession
    spark = SparkSession.builder.getOrCreate()

# -----------------------------
# Resolve and normalize config
# -----------------------------
out_base = S3_OUTPUT.rstrip("/") + "/"
act_out  = out_base + "activities/"
lead_out = out_base + "leads/"

limit_lead_search = min(max(int(PAGE_SIZE), 1), 200)  # Close caps around 200
limit_activity    = min(max(int(PAGE_SIZE), 1), 100)  # Close caps at 100

today = dt.date.today()
if START_DATE:
    start_day = dt.datetime.strptime(START_DATE, "%Y-%m-%d").date()
else:
    start_day = today - dt.timedelta(days=1)  # default yesterday
if END_DATE:
    end_day = dt.datetime.strptime(END_DATE, "%Y-%m-%d").date()
else:
    end_day = today

if OUTPUT_FORMAT not in ("parquet", "json"):
    raise ValueError(f"OUTPUT_FORMAT must be parquet|json, got {OUTPUT_FORMAT!r}")

# -----------------------------
# Fetch API key from Secrets Manager (inline)
# -----------------------------
sm = boto3.client("secretsmanager", region_name=REGION)
_secret_str = sm.get_secret_value(SecretId=SECRET_ID)["SecretString"]

api_key = None
try:
    _parsed = json.loads(_secret_str)
    if isinstance(_parsed, dict):
        api_key = _parsed.get("api_key") or _parsed.get("API_KEY")
    elif isinstance(_parsed, str):
        api_key = _parsed
except json.JSONDecodeError:
    api_key = _secret_str

if not api_key:
    raise RuntimeError(f"Secret {SECRET_ID} did not contain an API key")
api_key = api_key.strip().strip('"').strip("'")
if api_key.endswith(":"):
    api_key = api_key[:-1]
if not api_key:
    raise RuntimeError("API key is empty after normalization")

# -----------------------------
# Build auth headers (inline)
# -----------------------------
token = base64.b64encode(f"{api_key}:".encode("utf-8")).decode("ascii")
headers = {
    "Authorization": f"Basic {token}",
    "Accept": "application/json",
    "Content-Type": "application/json",
    "User-Agent": "glue-closeio-ingest/1.0"
}
print(f"[AUTH] Using API key len={len(api_key)} fingerprint={api_key[:4]}…{api_key[-3:]}")

# -----------------------------
# HTTP client + verify auth
# -----------------------------
http = urllib3.PoolManager()

# GET /me/ with light retry for 429/5xx
_auth_url = API_BASE_URL.rstrip("/") + "/me/"
_attempt = 0
while True:
    _resp = http.request("GET", _auth_url, headers=headers,
                         timeout=urllib3.Timeout(connect=10.0, read=30.0),
                         retries=False)
    if _resp.status in (429,) or _resp.status >= 500:
        _attempt += 1
        if _attempt > 6:
            raise RuntimeError(f"Auth check failed after retries: {_resp.status} {_resp.data[:200]!r}")
        _delay = min(0.5 * (2 ** (_attempt - 1)), 16.0) * (0.5 + random.random())
        print(f"[WARN] /me/ {_resp.status}, retry in ~{_delay:.2f}s")
        time.sleep(_delay)
        continue
    if _resp.status != 200:
        raise RuntimeError(f"Auth check failed: {_resp.status} {_resp.data[:200]!r}")
    break
print("[AUTH] OK")

print(f"[START] window={start_day}..{end_day} page_size={PAGE_SIZE} max_pages={MAX_PAGES} "
      f"details={FETCH_LEAD_DETAILS} limit={LEAD_LIMIT} fmt={OUTPUT_FORMAT} out={out_base}")

# -----------------------------
# Collect lead IDs (cursor pagination, NO functions)
# -----------------------------
all_leads: List[str] = []
_day = start_day
while _day <= end_day:
    _cursor = None
    _page = 0
    while _page < MAX_PAGES:
        _params = {
            "_limit": limit_lead_search,
            "query": {
                "negate": False,
                "queries": [
                    {"negate": False, "object_type": "lead", "type": "object_type"},
                    {
                        "negate": False,
                        "queries": [
                            {
                                "negate": False,
                                "queries": [
                                    {
                                        "condition": {
                                            "before":      {"type": "fixed_local_date", "value": _day.strftime("%Y-%m-%d"), "which": "end"},
                                            "on_or_after": {"type": "fixed_local_date", "value": _day.strftime("%Y-%m-%d"), "which": "start"},
                                            "type": "moment_range"
                                        },
                                        "field": {"field_name": "date_updated", "object_type": "lead", "type": "regular_field"},
                                        "negate": False,
                                        "type": "field_condition"
                                    }
                                ],
                                "type": "and"
                            }
                        ],
                        "type": "and"
                    }
                ],
                "type": "and"
            },
            "sort": []
        }
        if _cursor:
            _params["cursor"] = _cursor

        # POST /data/search/ with retry for 429/5xx
        _attempt = 0
        while True:
            _resp = http.request(
                "POST",
                API_BASE_URL.rstrip("/") + "/data/search/",
                body=json.dumps(_params).encode("utf-8"),
                headers=headers,
                timeout=urllib3.Timeout(connect=10.0, read=60.0),
                retries=False
            )
            if _resp.status in (429,) or _resp.status >= 500:
                _attempt += 1
                if _attempt > 6:
                    raise RuntimeError(f"POST /data/search failed after retries: {_resp.status} {_resp.data[:200]!r}")
                _delay = min(0.5 * (2 ** (_attempt - 1)), 16.0) * (0.5 + random.random())
                print(f"[WARN] search {_resp.status}, retry in ~{_delay:.2f}s")
                time.sleep(_delay)
                continue
            if _resp.status >= 400:
                raise RuntimeError(f"POST /data/search failed: {_resp.status} {_resp.data[:200]!r}")
            _payload = json.loads(_resp.data)
            break

        _data = _payload.get("data", [])
        _ids = [it["id"] for it in _data if it.get("__object_type") == "lead"]
        all_leads.extend(_ids)
        _cursor = _payload.get("cursor")
        _page += 1
        if not _cursor:
            break

    print(f"[LEADS] day={_day} fetched={len(_ids)} total={len(all_leads)}")
    _day += dt.timedelta(days=1)

# de-dup while preserving order
_seen = set()
deduped: List[str] = []
for _lid in all_leads:
    if _lid not in _seen:
        _seen.add(_lid)
        deduped.append(_lid)
if LEAD_LIMIT > 0:
    deduped = deduped[:LEAD_LIMIT]
print(f"[LEADS] unique={len(deduped)}")

# -----------------------------
# Fetch activities (+ optional lead details) with pagination; write in batches
# -----------------------------
act_buffer: List[Dict[str, Any]] = []
lead_buffer: List[Dict[str, Any]] = []
total_act = 0
total_lead = 0

for _idx, _lead_id in enumerate(deduped, 1):
    try:
        # ---- Activities pagination (limit<=100, walk back by activity_at__lt)
        _first = True
        _cursor_time = None
        while True:
            _params: Dict[str, Any] = {"lead_id": _lead_id, "limit": limit_activity}
            if not _first and _cursor_time:
                _params["activity_at__lt"] = _cursor_time

            # GET /activity/ with retry
            _attempt = 0
            while True:
                _resp = http.request(
                    "GET",
                    API_BASE_URL.rstrip("/") + "/activity/",
                    headers=headers,
                    fields=_params,  # urllib3 will add as query params
                    timeout=urllib3.Timeout(connect=10.0, read=60.0),
                    retries=False
                )
                if _resp.status in (429,) or _resp.status >= 500:
                    _attempt += 1
                    if _attempt > 6:
                        raise RuntimeError(f"GET /activity failed after retries: {_resp.status} {_resp.data[:200]!r}")
                    _delay = min(0.5 * (2 ** (_attempt - 1)), 16.0) * (0.5 + random.random())
                    print(f"[WARN] activity {_resp.status}, retry in ~{_delay:.2f}s")
                    time.sleep(_delay)
                    continue
                if _resp.status >= 400:
                    raise RuntimeError(f"GET /activity failed: {_resp.status} {_resp.data[:200]!r}")
                _payload = json.loads(_resp.data)
                break

            _data = _payload.get("data", [])
            if not _data:
                break

            for _it in _data:
                if "lead_id" not in _it:
                    _it["lead_id"] = _lead_id
            act_buffer.extend(_data)

            if not _payload.get("has_more", False):
                break

            _cursor_time = _data[-1].get("activity_at") or _data[-1].get("date_created")
            _first = False

        # ---- Optional lead detail
        if FETCH_LEAD_DETAILS:
            _attempt = 0
            while True:
                _resp = http.request(
                    "GET",
                    API_BASE_URL.rstrip("/") + f"/lead/{_lead_id}/",
                    headers=headers,
                    timeout=urllib3.Timeout(connect=10.0, read=60.0),
                    retries=False
                )
                if _resp.status in (429,) or _resp.status >= 500:
                    _attempt += 1
                    if _attempt > 6:
                        raise RuntimeError(f"GET /lead/{{id}} failed after retries: {_resp.status} {_resp.data[:200]!r}")
                    _delay = min(0.5 * (2 ** (_attempt - 1)), 16.0) * (0.5 + random.random())
                    print(f"[WARN] lead detail {_resp.status}, retry in ~{_delay:.2f}s")
                    time.sleep(_delay)
                    continue
                if _resp.status >= 400:
                    raise RuntimeError(f"GET /lead/{{id}} failed: {_resp.status} {_resp.data[:200]!r}")
                _detail = json.loads(_resp.data)
                break

            lead_buffer.append(_detail)

        # ---- Periodic flushes (activities)
        if len(act_buffer) >= ACT_FLUSH_EVERY:
            _rdd = spark.sparkContext.parallelize([json.dumps(r) for r in act_buffer])
            _df = spark.read.json(_rdd)
            _now = F.current_timestamp()
            _df = _df.withColumn("ingest_ts_utc", _now).withColumn("ingest_date", F.to_date(_now))
            if OUTPUT_FORMAT == "parquet":
                _df.write.mode("append").partitionBy("ingest_date").parquet(act_out)
            else:
                _df.write.mode("append").partitionBy("ingest_date").json(act_out)
            _written = _df.count()
            total_act += _written
            act_buffer.clear()
            print(f"[WRITE] activities batch={_written} total={total_act}")

        # ---- Periodic flushes (lead details)
        if FETCH_LEAD_DETAILS and len(lead_buffer) >= LEAD_FLUSH_EVERY:
            _rdd = spark.sparkContext.parallelize([json.dumps(r) for r in lead_buffer])
            _df = spark.read.json(_rdd)
            _now = F.current_timestamp()
            _df = _df.withColumn("ingest_ts_utc", _now).withColumn("ingest_date", F.to_date(_now))
            if OUTPUT_FORMAT == "parquet":
                _df.write.mode("append").partitionBy("ingest_date").parquet(lead_out)
            else:
                _df.write.mode("append").partitionBy("ingest_date").json(lead_out)
            _written = _df.count()
            total_lead += _written
            lead_buffer.clear()
            print(f"[WRITE] leads batch={_written} total={total_lead}")

        if _idx % 100 == 0:
            print(f"[PROGRESS] leads processed {_idx}/{len(deduped)}")

    except Exception as e:
        print(f"[ERROR] lead_id={_lead_id}: {e}")
        print(traceback.format_exc())

# -----------------------------
# Final flush
# -----------------------------
if act_buffer:
    _rdd = spark.sparkContext.parallelize([json.dumps(r) for r in act_buffer])
    _df = spark.read.json(_rdd)
    _now = F.current_timestamp()
    _df = _df.withColumn("ingest_ts_utc", _now).withColumn("ingest_date", F.to_date(_now))
    if OUTPUT_FORMAT == "parquet":
        _df.write.mode("append").partitionBy("ingest_date").parquet(act_out)
    else:
        _df.write.mode("append").partitionBy("ingest_date").json(act_out)
    _written = _df.count()
    total_act += _written
    print(f"[WRITE] activities final={_written} total={total_act}")

if FETCH_LEAD_DETAILS and lead_buffer:
    _rdd = spark.sparkContext.parallelize([json.dumps(r) for r in lead_buffer])
    _df = spark.read.json(_rdd)
    _now = F.current_timestamp()
    _df = _df.withColumn("ingest_ts_utc", _now).withColumn("ingest_date", F.to_date(_now))
    if OUTPUT_FORMAT == "parquet":
        _df.write.mode("append").partitionBy("ingest_date").parquet(lead_out)
    else:
        _df.write.mode("append").partitionBy("ingest_date").json(lead_out)
    _written = _df.count()
    total_lead += _written
    print(f"[WRITE] leads final={_written} total={total_lead}")

print(f"[DONE] total_activities={total_act} total_leads={total_lead}")
